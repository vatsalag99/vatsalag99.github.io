<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor">
  <meta name="keywords" content="Stable Diffusion, Multimodal Understanding, Computer Vision, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">


  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <!-- <link rel="icon" href="./static/images/favicon.png"> -->


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <!-- <script src="./static/js/loop_all_vids.js"></script> -->
  <!-- <script src="./static/js/index.js"></script> -->



  <style>
    ol#horizontal_list li {
      display: inline;
    }


    ol.horizontal {
      list-style-type: decimal;
      width: 600px;
    }

    ol.horizontal li {
      float: left;
      width: 200px;
      padding: 2px 0px;
    }

    /*
/* Tooltip container */
    .tooltip {
      position: relative;
      display: inline-block;
      border-bottom: 1px dotted black;
      /* If you want dots under the hoverable text */
    }

    .tooltip {
      display: inline-block;
      position: relative;
      border-bottom: 1px dotted #666;
      text-align: left;
    }

    .tooltip h3 {
      margin: 12px 0;
    }

    .tooltip .right {
      min-width: 200px;
      max-width: 400px;
      top: 50%;
      left: 100%;
      margin-left: 20px;
      transform: translate(0, -50%);
      padding: 0;
      color: #EEEEEE;
      background-color: #444444;
      font-weight: normal;
      font-size: 13px;
      border-radius: 8px;
      position: absolute;
      z-index: 99999999;
      box-sizing: border-box;
      box-shadow: 0 1px 8px rgba(0, 0, 0, 0.5);
      visibility: hidden;
      opacity: 0;
      transition: opacity 0.8s;
    }

    .tooltip:hover .right {
      visibility: visible;
      opacity: 1;
    }

    .tooltip .right img {
      width: 400px;
      border-radius: 8px 8px 0 0;
    }

    .tooltip .text-content {
      padding: 10px 20px;
    }

    .tooltip .right i {
      position: absolute;
      top: 50%;
      right: 100%;
      margin-top: -12px;
      width: 12px;
      height: 24px;
      overflow: hidden;
    }

    .tooltip .right i::after {
      content: '';
      position: absolute;
      width: 12px;
      height: 12px;
      left: 0;
      top: 50%;
      transform: translate(50%, -50%) rotate(-45deg);
      background-color: #444444;
      box-shadow: 0 1px 8px rgba(0, 0, 0, 0.5);
    }
  </style>


</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> 
    </div>

  </div>
</nav> -->


  <section class="hero">
    <!-- <div class="hero-body"> -->
    <div class="container is-max-desktop">
      <!-- <div class="columns is-centered"> -->
      <div class="column has-text-centered">
        <h1 class="title is-1 publication-title">Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor</h1>

        <div class="is-size-5 publication-authors">
          <span class="author-block">
            <a href="https://vatsalag99.github.io/">Vatsal Agarwal</a><sup>*1</sup>,
          </span>
          <span class="author-block">
            <a href="https://mgwillia.github.io/">Matthew Gwilliam</a><sup>1</sup>,
          </span>
          <span class="author-block">
            <a href="https://www.linkedin.com/in/gefenkohavi">Gefen Kohavi</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=2dDfKwUAAAAJ&hl=en">Eshan Verma</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a href="https://scholar.google.com/citations?user=zIXl1-QAAAAJ&hl=en">Daniel Ulbricht</a><sup>2</sup>,
          </span>
          <span class="author-block">
            <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a><sup>1</sup>
          </span>
        </div>

        <div class="is-size-5 publication-authors">
          <span class="author-block"><sup>1</sup>University of Maryland,</span>
          <span class="author-block"><sup>2</sup>Apple</span>
          <span class="author-block"><sup>*</sup>Internship at Apple</span>
        </div>

        <!-- <div class="is-size-5 publication-authors">
          <span class="author-block">CVPR 2024</span>
        </div> -->

        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- PDF Link. -->
            <span class="link-block">
              <!-- TODO: change the paper link  -->
              <a href="https://arxiv.org/pdf/2401.10217.pdf" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
            <!-- arXiv Link. -->
            <span class="link-block">
              <!-- TODO: change the arXiv link  -->
              <a href="https://arxiv.org/abs/2401.10217" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>
            <!-- Video Link. -->
            <!-- <span class="link-block">
                <a href="#in-the-wild"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Videos</span>
                </a>
              </span> -->
            <!-- Code Link. -->
            <!-- <span class="link-block">
              <a href="https://github.com/namithap10/xinc" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span> -->
            <!--              <!~~ Dataset Link. ~~>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>-->
          </div>

        </div>
      </div>
      <!-- </div> -->
    </div>
    <!-- </div> -->
  </section>

  <!--<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>-->

  <!-- <ul id="website_vids"></ul> -->

  <section class="section">
    <div class="container  is-max-desktop has-text-centered">
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- <span class="dnerf">We present <b>mustafar</b>, a multimodal understanding framework that leverages Stable Diffusion as a task-aware feature extractor. </span> -->
      <!-- </h2> -->

      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent advances in multimodal large language models (MLLMs) have enabled
              image-based question-answering capabilities. However, a key limitation is the use
              of CLIP as the visual encoder; while it can capture coarse global information, it
              often can miss fine-grained details that are relevant to the input query. To address
              these shortcomings, this work studies whether pre-trained text-to-image diffusion
              models can serve as instruction-aware visual encoders. Through an analysis of their
              internal representations, we find diffusion features are both rich in semantics and
              can encode strong image-text alignment. Moreover, we find that we can leverage
              text conditioning to focus the model on regions relevant to the input question. We
              then investigate how to align these features with large language models and uncover
              a leakage phenomenon, where the LLM can inadvertently recover information from
              the original diffusion prompt. We analyze the causes of this leakage and propose a
              mitigation strategy. Based on these insights, we explore a simple fusion strategy that
              utilizes both CLIP and conditional diffusion features. We evaluate our approach on
              both general VQA and specialized MLLM benchmarks, demonstrating the promise
              of diffusion models for visual understanding, particularly in vision-centric tasks
              that require spatial and compositional reasoning.
            </p>
          </div>
        </div>
      </div>
  </section>

  <br>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <h2 class="title is-3 has-text-centered">Overview</h2>

          <!-- Teaser -->
          <div class="container  is-max-desktop has-text-centered">
            <img id="teaser" src="./static/images/Teaser.png" alt="Teaser" />
          </div>


          <br>

          <p class="content has-text-justified">
            We present <b>Mustafar</b>, a multimodal understanding framework that leverages Stable Diffusion as a task-aware feature extractor. Motivated by the weaknesses of CLIP in capturing fine-grained and compositional visual information, we explore whether the internal representations of text-to-image diffusion models can be harnessed for visual question answering (VQA). We begin by analyzing diffusion features across a range of multimodal tasks and find that they offer both rich semantics and strong image-text alignment. Our investigations into text conditioning reveal that increasing the strength of text guidance leads to sharper focus on relevant image regions. However, we also discover that this alignment can result in caption leakage, where downstream language models are able to recover the  captions from the diffusion features. We provide a detailed analysis of this phenomenon and propose a mitigation strategy to reduce leakage. We further examine using questions directly as prompts and show, through both qualitative and quantitative analysis, that question conditioning helps highlight task-relevant image regions. Finally, we demonstrate that simple fusion strategies combining CLIP features with question-conditioned diffusion features can achieve strong performance across general-purpose and vision-centric benchmarks. Our results suggest that diffusion models are a promising direction for advancing visual understanding, particularly in tasks that demand fine-grained spatial reasoning and compositionality.
          </p>
        </div>
      </div>
  </section>

  <br>

  <!-- Feature Visualization -->
  <section class="hero teaser" id="feature_viz">
    <div class="container is-max-desktop">
      <h3 class="title is-3 has-text-centered">What Visual Information Do Diffusion Models Encode?</h3>


      <br>
      <h3 class="title is-4 has-text">Diffusion Features Encode Semantic and Structural Information</h3>
      
      <img src="./static/images/Figure2.png" class="interpolation-image" alt="unconditional features." />

      <br>
      <p class="has-text-justified">
        We first extract and inspect unconditional features using Stable Diffusion v2.1 as our diffusion model. We utilize PCA for dimensionality reduction to visualize the features, with each channel representing a principal component. Our visualizations reveal three key insights about diffusion features:
        <br><br>
        <strong>Feature Diversity:</strong> Features from different blocks capture both shared semantics and image-specific details. Additionally, features like <code>out</code> and <code>res-out</code> contain tokens that act as "registers" - shared global descriptors across similar images.
        <br><br>
        <strong>Timestep Behavior:</strong> Higher timesteps encode coarse layout, while lower timesteps emphasize fine-grained structure.
        <br><br>
        <strong>Similarity Analysis:</strong> To understand how well diffusion features encode visual differences, we plot average cosine similarity between pairs of similar images. We observe that: (1) diffusion features capture intra-pair visual differences better than CLIP; (2) cross-q features show higher pairwise similarity than b0-out and res-out features; and (3) pairwise similarity decreases as timestep increases.
      </p>

      <!-- <br>
      <br> -->

      <!-- <h5 class="title is-3 has-text-centered">Diffusion Features are Effective for Vision-Centric Tasks</h5>
      <img src="./static/images/Table1.png" class="interpolation-image" alt="unconditional features." />


      <br>
      <br>

      <p class="has-text-justified">
       
        We evaluate multimodal reasoning using the LLaVA framework with diffusion features at different layers and timesteps. Our results show that diffusion features are able to match CLIP performance on vision-centric benchmarks such as MMVP and BLINK-val, while faring worse on general-purpose benchmarks that require more vision-language alignment such as LLaVA-Bench. Delving deeper into the performance of specific layers and timesteps, we observe that  <code>cross-q</code> features generally perform better than <code>out</code> features. Looking at timesteps, we observe that features extracted at earlier timesteps between  <code>t=10</code> and <code>t=100</code> perform well compared to earlier or later timesteps. 
      </p>       -->

  <br>
  <br>

  <h3 class="title is-4 has-text">Cross-Attention Maps Capture Text-Aligned Visual Semantics</h3>

    <img src="./static/images/Figure4.png" class="interpolation-image" alt="unconditional features." />

    <br>
    <p class="has-text-justified">
      
      We first visualize cross-attention maps to identify how well diffusion models capture image-text alignment. We show two samples from the COCO-captions dataset and their averaged cross-attention maps. Cross-attention maps at higher timesteps show higher focus on background elements (e.g., “court”). Attention maps from lower timesteps provide improved localization of both object and action concepts (e.g., “racquets” and “holding”). Overall, we see robust correspondence with precise localization of regions relevant to objects and actions. 
    </p>

    <br>
    <br>


    <h3 class="title is-4 has-text"> Text Conditioning Modulates Spatial Feature Representations</h3>

    <img src="./static/images/Figure5.png" class="interpolation-image" alt="unconditional features." />

    <br>
    <br>
    <p class="has-text-justified">
      
      We next investigate how these cross-attention maps impact conditional diffusion features. We select the images from the MMVP-
      VLM dataset and inspect PCA maps for both unconditional and conditional features. Our qualitative analysis shows that while conditional features resemble unconditional features structurally, the difference between them highlight caption-relevant regions such as the elephan's trunk or the minion's tongue. A CKA analysis between unconditional and conditional features at varying guidance scales shows that increasing guidance leads to greater text modulation. 

    </p>

    <br>
    <br>

    <h3 class="title is-4 has-text">Amplified Text Guidance Enables Leakage</h3>

    <img src="./static/images/Figure6.png" class="interpolation-image" alt="unconditional features." />

    <br>
    <br>

    <p class="has-text-justified">

      Motivated by our findings, we explore whether increasing text-guidance can aid in aligning diffusion features to a
      downstream language model. Using image-captioning as a proxy for measuring alignment, we find models trained with stronger ground-
      truth conditioning outperform the CLIP baseline when given ground-truth captions at inference but degrade significantly when no caption is provided. This indicates the potential of caption leakage occuring where the language model is simply able to reconstruct the caption from the diffusion features. We confirm this by passing a mismatched caption during inference and observing if the model generates the passed-in caption or one corresponding to the image. To mitigate this, we propose passing no input-captions randomly (denoted as dropout) and find that this alleviates the leakage phenomenon.
    </p>
</section>

<br>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <h2 class="title is-3 has-text-centered">Can We Extract Task-Aware Features for Question-Answering?</h2>

        <!-- Teaser -->
        <div class="container  is-max-desktop has-text-centered">
          <img id="teaser" src="./static/images/Figure7.png" alt="Teaser" />
        </div>


        <br>
        <br>

        <p class="content has-text-justified">
          Yes! We first identify that passing questions as the text-prompt can enable the model to focus on relevant regions. To mitigate the leakage effect further, we perform pre-training with no input text-prompt and only utilize questions during supervised fine-tuning. To harness the benefits of both CLIP and diffusion features, we experiment with two simple fusion strategies: (1) concatenation and (2) cross-attention. Our results show that both fusion strategies are able to match or exceed the performance of the CLIP baseline on general-purpose and vision-centric benchmarks.
        </p>
      </div>
    </div>
</section>


  <!-- <script src="./static/js/loop_all_vids.js"></script> -->

  <!-- <section class="section">
    <div class="container is-max-desktop"> -->

      <!-- Animation. -->
      <!--    <div class="columns is-centered">
    <!~~ <div class="columns "> ~~>
      <div class="column has-text-centered is-full-width">
      <!~~ <div class="column has-text-centered  is-three-fifths"> ~~>
        <h2 class="title is-3">Diffusion Process: Intermediate States</h2>

        <!~~ Interpolating. ~~>
        <!~~ <h3 class="title is-4">Diffusion Process: Intermediate States</h3> ~~>
        <!~~ <div class="content has-text-centered is-centred has-text-justified"> ~~>
          <div class="content has-text-centered is-centred ">
          <p>
            Use the slider here to iteratively denoise left frame to the right
            frame.
          </p>
        </div>
        <!~~ <div class="columns is-three-fifths"> ~~>
          <div class="columns is-centered is-fullwidth ">
          <div class="columns is-centered is-vcentered interpolation-panel">
          <!~~ <div class="columns is-vcentered"> ~~>
            <div class="column is-3 has-text-centered">
              <img src="./static/images/interpolate_start.jpg"
                  class="interpolation-image"
                  alt="Interpolate start reference image."/>
              <p>Masked Frame</p>
            </div>
            <div class="column is-3 is-centered has-text-centered interpolation-video-column">
              <div id="interpolation-image-wrapper">
                Loading...
              </div>
              <input class="slider is-fullwidth is-large is-info"
                    id="interpolation-slider"
                    step="1" min="0" max="100" value="0" type="range">
            </div>
            <div class="column is-3 has-text-centered">
              <img src="./static/images/interpolate_end.jpg"
                  class="interpolation-image"
                  alt="Interpolation end reference image."/>
              <p class="is-bold">Generated Frame</p>
            </div>
          </div>
        </div>
        <br/>
        <!~~/ Interpolating. ~~>
      </div>
    </div>-->
      <!--/ Animation. -->

      <!-- Animation. -->
      <!-- <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Diffusion Process Visualization</h2>

          Interpolating.
          <h3 class="title is-4">Intermediate States</h3>
          <div class="content has-text-justified">
            <p>
              Use the slider here to iteratively denoise left frame to the right
              frame.
            </p>
          </div>

          <div class="columns">
            <div class="column is-8 is-offset-2">
              <div class="columns is-vcentered interpolation-panel">


                <div class="column is-3 has-text-centered">
                  <img src="./static/images/interpolate_start.jpg" class="interpolation-image"
                    alt="Interpolate start reference image." />
                  <p>Masked Frame</p>
                </div>
                <div class="column  interpolation-video-column">
                  <div class="columns is-centered ">
                    <div id="interpolation-image-wrapper" class="columns is-centered">
                      Loading...
                    </div>
                  </div>
                  <div class="columns is-centered">
                    <div class="column is-three-fifths">
                      <input class="slider is-half is-centered is-large is-info" id="interpolation-slider" step="1"
                        min="0" max="100" value="0" type="range">
                    </div>
                  </div>
                </div>
                <div class="column is-3 has-text-centered">
                  <img src="./static/images/interpolate_end.jpg" class="interpolation-image"
                    alt="Interpolation end reference image." />
                  <p class="is-bold">Generated Frame</p>
                </div>


              </div>
            </div>
          </div>


          <br />
          / Interpolating.

        </div>
      </div> -->
      <!--/ Animation. -->

      <!-- Related Work. -->
      <!-- <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Work</h2>

          <ol>

            <li>
              <p><a href="https://arxiv.org/abs/2112.10752">Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-Resolution Image Synthesis with Latent Diffusion Models. In <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2022.</a></p>
            </li>
            <li>
              <p><a href="https://arxiv.org/abs/2203.05423">Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning Transferable Visual Models From Natural Language Supervision. In <i>International Conference on Machine Learning (ICML)</i>, 2021.</a></p>
            </li>
            <li>
              <p><a href="https://arxiv.org/abs/2204.01691">Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. CoCa: Contrastive Captioners are Image-Text Foundation Models. In <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, 2022.</a></p>
            </li>
            <li>
              <p><a href="https://arxiv.org/abs/2303.08774">Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J., and Norouzi, M. Image Super-Resolution via Iterative Refinement. In <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 2023.</a></p>
            </li>
          </ol>

        </div>
      </div>
    </div> -->
    <!--/ Related Work. -->
<!-- 
    </div>
  </section> -->

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{agarwal2024multimodal,
          author    = {Agarwal, Vatsal and Gwilliam, Matthew and Kohavi, Gefen and Verma, Eshan and Ulbricht, Daniel and Shrivastava, Abhinav},
          title     = {Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor},
          journal   = {arXiv preprint},
          year      = {2024},
        }
      </code></pre>
    </div>
  </section> -->

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p class="has-text-centered">
              <!-- This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>. -->
              The source code of this webpage is based on the <a
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project webpage.
            </p>
            <p>

          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>